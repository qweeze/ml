{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### requirements\n",
    "```\n",
    "fasttext==0.9.2\n",
    "gensim==3.6.0\n",
    "nltk==3.4\n",
    "pandas==0.23.4\n",
    "tqdm==4.47.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def read_vacancies_part(part_num):\n",
    "    with gzip.open(f'vacancies-{part_num:02}.json.gz') as fp:\n",
    "        return json.loads(fp.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing import preprocessing\n",
    "\n",
    "stop_words = frozenset(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def generate_features(data):\n",
    "    yield data['employer'][:7]\n",
    "\n",
    "    yield f'ar{data[\"area_id\"]}'\n",
    "\n",
    "    sal_lo, sal_hi = data['compensation_from'], data['compensation_to']\n",
    "    if sal_lo:\n",
    "        yield f'lo{sal_lo // 10_000}'\n",
    "    if sal_hi:\n",
    "        yield f'hi{sal_hi // 10_000}'\n",
    "    if sal_lo and sal_hi:\n",
    "        yield f'spr{(sal_hi - sal_lo) // 10_000}'\n",
    "\n",
    "    yield data['work_schedule'] or 'nosch'\n",
    "\n",
    "    yield data['currency'] or 'rur'\n",
    "\n",
    "    yield ' '.join([data['name']] * 5)\n",
    "\n",
    "    yield from data['key_skills']\n",
    "\n",
    "    yield data['description']\n",
    "        \n",
    "\n",
    "def extract_text(vacancy_data):\n",
    "    text = ' '.join(generate_features(vacancy_data))\n",
    "    \n",
    "    return ' '.join(\n",
    "        filter(\n",
    "            lambda t: t not in stop_words,\n",
    "            preprocessing.preprocess_string(\n",
    "                text.replace('\\n', ' '),\n",
    "                filters=(\n",
    "                    lambda t: t.lower(),\n",
    "                    preprocessing.strip_tags,\n",
    "                    preprocessing.strip_punctuation,\n",
    "                    preprocessing.strip_multiple_whitespaces,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_specializations = {\n",
    "    vacancy_id: list(map(int, specs[1:-1].split(',')))\n",
    "    for vacancy_id, specs in (\n",
    "        pd.read_csv('train_labels.csv.gz', compression='gzip')\n",
    "        .set_index('vacancy_id')['specializations']\n",
    "        .iteritems()\n",
    "    )\n",
    "}\n",
    "test_ids = pd.read_csv('test_vacancy_ids.csv.gz', compression='gzip').values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm\n",
    "\n",
    "with open('train.txt', 'w') as fp, tqdm(total=len(train_specializations)) as pbar:\n",
    "    for part_num in range(1, 11):\n",
    "        part = read_vacancies_part(part_num)\n",
    "        \n",
    "        for key, data in part.items():\n",
    "            vacancy_id = int(key)\n",
    "            if vacancy_id not in train_specializations:\n",
    "                continue\n",
    "\n",
    "            labels = train_specializations[vacancy_id]\n",
    "            text = extract_text(data)\n",
    "\n",
    "            print(\n",
    "                *(f'__label__{label}' for label in labels),\n",
    "                text,\n",
    "                file=fp,\n",
    "            )\n",
    "\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import fasttext\n",
    "\n",
    "model = fasttext.train_supervised(\n",
    "    input='train.txt',\n",
    "    lr=0.65,\n",
    "    epoch=15,\n",
    "    wordNgrams=4,\n",
    "    dim=150,\n",
    "    loss='ova',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = {}\n",
    "\n",
    "with tqdm(total=len(test_ids)) as pbar:\n",
    "    for part_num in range(1, 11):\n",
    "        part = read_vacancies_part(part_num)\n",
    "        for key, data in part.items():\n",
    "            vacancy_id = int(key)\n",
    "            if vacancy_id not in test_ids:\n",
    "                continue\n",
    "            \n",
    "            text = extract_text(data)\n",
    "            labels, _ = model.predict(text, k=6, threshold=0.13)\n",
    "            if not labels:\n",
    "                labels, _ = model.predict(text, k=3)\n",
    "\n",
    "            predicted[vacancy_id] = [int(label.split('__label__')[1]) for label in labels]\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame([\n",
    "    (vacancy_id, predicted[vacancy_id])\n",
    "    for vacancy_id in test_ids\n",
    "], columns=['vacancy_id', 'specializations'])\n",
    "\n",
    "submission.to_csv('submission.csv.gz', index=False, compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "'Python Interactive'",
   "language": "python",
   "name": "69a203eb-7e6a-4756-bfd3-f22872c33e65"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
